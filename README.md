ğŸ“˜ SIGN.MT â€“ Team Collaboration Documentation (Updated)

A clean, technical, collaboration-friendly document

1. Project Summary

sign.mt is a multilingual sign-language translation system that converts:

Text â†’ Sign Video

Speech â†’ Text â†’ Sign Video

YouTube Audio â†’ Text â†’ Sign Video

This version of the project does NOT use any trained machine-learning model.
We have NOT trained on any dataset yet.

Instead, the system uses:

Dummy gloss generation

Dictionary-based gloss â†’ pose mapping

Lightweight OpenCV-based video rendering

This makes the project fast, simple, and able to run on low-resource devices.

2. Current Achievements (What works now)
âœ” Multilingual text translation

Any language â†’ English (deep-translator)

âœ” Speech recognition

Microphone input

YouTube audio input

âœ” Dummy gloss generation

Simple uppercase token glossing.
(No ML training used.)

âœ” Gloss â†’ Pose mapping

Dictionary-based synthetic pose generation.
(No real dataset used.)

âœ” Video rendering

Lightweight

Low-resolution

Optimized performer

Outputs output.mp4

âœ” Runs on basic laptops

No GPU required

Python 3.10

Low RAM usage

â— Important: No ML models are trained in this project yet

No textâ†’gloss ML model

No glossâ†’pose ML model

No dataset has been trained on

Only pre-coded rule-based methods

This is clearly stated for team transparency.

3. Future Goals (What we want to achieve)
ğŸŸ¦ 1. Train real Text â†’ Gloss model

Using datasets like:

RWTH-PHOENIX-2014T

How2Sign

ASLG-PC12

(Model: Transformer / seq2seq)

ğŸŸ¦ 2. Train Gloss â†’ Pose generation model

Using:

Mediapipe extracted pose sequences

LSTM / Transformer

ğŸŸ¦ 3. Add 3D pose and avatar animation

(Not included yet.)

ğŸŸ¦ 4. Add proper sign grammar rules
ğŸŸ¦ 5. Collect + preprocess datasets

Team members can help with this step.

4. Project Folder Structure (With Explanation of Missing ML Training)
SIGNMT/
â”‚ demo_run.py                â†’ Main pipeline (runs text/speech/video input)
â”‚ output.mp4                 â†’ Final generated video
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ text_clean.py        â†’ Text preprocessing
â”‚   â”œâ”€â”€ segmentation.py      â†’ Sentence splitting
â”‚   â”œâ”€â”€ langid.py            â†’ Language detection
â”‚   â”œâ”€â”€ gloss_to_pose_dict.pyâ†’ Dictionary-based pose generator (NO ML)
â”‚   â”œâ”€â”€ pose_render.py       â†’ Video rendering (OpenCV)
â”‚   â”œâ”€â”€ stt.py               â†’ Speech-to-text
â”‚   â””â”€â”€ youtube_audio.py     â†’ YouTube audio extraction
â”‚
â”œâ”€â”€ models/                  â†’ (Currently contains no trained ML models)
â”‚                            â†’ (Textâ†’gloss model NOT trained yet)
â”‚                            â†’ (Glossâ†’pose neural model NOT trained yet)
â”‚

âš  Note for team:

models/ folder contains placeholders.
No ML training has been done.
All behavior is rule-based.

5. How the System Works (For New Team Members)
âœ” 1. Input selection

User chooses:

Text

Microphone speech

YouTube audio

âœ” 2. Language Detection

langdetect identifies the language.

âœ” 3. Translate to English

We use GoogleTranslator.

âœ” 4. Text Cleaning

Lowercasing + punctuation removal.

âœ” 5. Gloss Generation (Dummy)

Words â†’ UPPERCASE tokens
(No trained gloss model is used.)

âœ” 6. Gloss â†’ Pose Generation

Using a static dictionary mapping
(No ML model involved.)

âœ” 7. Video Rendering

OpenCV draws white pose dots â†’ saves as output.mp4.

6. Prerequisites (Team Member Setup)
âœ” Python 3.10

(Important!)

âœ” Required packages
pip install numpy==1.26.4
pip install langdetect
pip install deep-translator
pip install opencv-python
pip install yt-dlp
pip install sounddevice
pip install nltk
pip install rich
pip install certifi


(Optional STT)

pip install openai-whisper

System Requirements:

Windows 10/11

4GB RAM

No GPU needed

7. Running the Project

Step 1 â€” Activate environment:

signmt-env\Scripts\Activate.ps1


Step 2 â€” Run main file:

python demo_run.py


Step 3 â€” Choose mode:

1 â†’ Text â†’ Sign
2 â†’ Microphone â†’ Sign
3 â†’ YouTube â†’ Sign


Step 4 â€” Output:

output.mp4

8. What The Team Should Know (Important Notes)
âœ” This project currently uses NO AI training

Just rule-based transformations.

âœ” All ML folders are placeholders

We will add real ML in Phase 2.

âœ” Everyone should understand the pipeline

so future improvements can be added easily.

âœ” The system is modular

Each component (translation, glossing, pose rendering) is separate.

9. Final Summary for Collaboration

This document is the official guide for contributors.
Everyone joining the project must know:

âœ” What the system currently does
âœ” What it does NOT do (NO TRAINED MODELS)
âœ” What the next goals are
âœ” How the code is structured
âœ” How to run it
âœ” What tasks are open for contributors

